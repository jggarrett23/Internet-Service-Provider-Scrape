{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yelpapi import YelpAPI\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir =  os.path.join(os.getcwd(),'Yelp_Data\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_api = YelpAPI(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part is going to be a pain. We can loop through this and get \n",
    "# results for each area, but there is a limit on the api request per day \n",
    "# per 30 days. Maybe just focus on major cities?\n",
    "\n",
    "businesses_location = 'Alturas, CA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can play around with the limit and offset parameters \n",
    "# to control the number of results and what item to start the pull on\n",
    "search_results = yelp_api.search_query(term='Internet Service Providers',\n",
    "                                       location=businesses_location,\n",
    "                                      limit = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df = pd.DataFrame.from_dict(search_results['businesses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yerp\n"
     ]
    }
   ],
   "source": [
    "if business_df.empty:\n",
    "    print('Yerp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the phone, display_phone, transactions, is_closed, and image_url columns\n",
    "# we shouldn't need them\n",
    "unecessary_cols = ['phone', 'display_phone', 'transactions', 'is_closed', \n",
    "                   'image_url']\n",
    "\n",
    "\n",
    "business_df2 = business_df.drop(unecessary_cols,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through businesses\n",
    "business_reviews = dict()\n",
    "for iBiz, biz_id in enumerate(business_df2.loc[:,'id']):\n",
    "    business_name = business_df2['name'][iBiz]\n",
    "    \n",
    "    #can only get 3 reviews through yelp api\n",
    "    #BUT...we have the url...which means it should be easy to \"not legally\" scrape\n",
    "    business_reviews[business_name] = yelp_api.reviews_query(biz_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "businessRev_df = pd.DataFrame()\n",
    "\n",
    "for biz in business_reviews.keys():\n",
    "    \n",
    "    # temporary data frame we can use that will be appended to a master one later\n",
    "    temp_df = pd.DataFrame.from_dict(business_reviews[biz]['reviews'])\n",
    "    \n",
    "    temp_df = temp_df.drop('user',1)\n",
    "    \n",
    "    # add column for ISP provider\n",
    "    temp_df.insert(0,'ISP_name',biz)\n",
    "    \n",
    "    # add column for business id\n",
    "    temp_df.insert(1,'business_id',\n",
    "                   business_df[business_df['name'] == biz]['id'].item())\n",
    "    \n",
    "    temp_df.insert(6,'location',\n",
    "                       str(business_df2[business_df2['name'] == biz]['location'].item()['display_address']))\n",
    "    \n",
    "    temp_df = temp_df.rename(columns = {\"id\":\"rev_id\"})\n",
    "    \n",
    "    businessRev_df = businessRev_df.append(temp_df,ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "business_df2.to_csv(data_dir+'businesses.csv', index=False)\n",
    "businessRev_df.to_csv(data_dir+'businesses_reviews.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape data using the yelp api\n",
    "\n",
    "# pip install yelp api\n",
    "from yelpapi import YelpAPI\n",
    "import pandas as pd\n",
    "import os \n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def Yelp_ScrapeISP (api_key,city_names,business_data = 'businesses.csv',\n",
    "                    business_reviews = 'businesses_reviews.csv'):\n",
    "\n",
    "    \"\"\"\n",
    "    ====================================================================\n",
    "    Version: 1.0.1\n",
    "    Date: Tue 24 Nov 2020\n",
    "\n",
    "    Purposes: Search and save yelp data about internet service providers\n",
    "    within a region. \n",
    "\n",
    "    Input:\n",
    "        Required:\n",
    "            api_key = Api key assigned by yelp fusion\n",
    "            city_names = List of locations for internet service providers\n",
    "        Opitional:\n",
    "            business_data = .csv file containing data from previous searches\n",
    "            business_reviews = .csv file containing reviews of businesses from\n",
    "                                previous searches\n",
    "\n",
    "    Output:\n",
    "        'businesses.csv' containing information about internet \n",
    "            service providers\n",
    "\n",
    "        'businesses_reviews.csv' containing reviews of internet service\n",
    "            providers in the businesses.csv file\n",
    "\n",
    "        'cities_list.txt' list of previous cities that have been searched.\n",
    "            Data will only be extracted for cities that have not been\n",
    "            previously searched.\n",
    "\n",
    "\n",
    "    Example: \n",
    "    ca_cities_df = pd.read_csv('cal_cities_lat_long.csv')\n",
    "    ca_cities = ca_cities_df['Name'] + ', CA'\n",
    "\n",
    "    api_key = XIXIXIXLXJO\n",
    "\n",
    "    Yelp_ScrapeISP(api_key,ca_cities)\n",
    "\n",
    "\n",
    "    Author: Jordan Garrett\n",
    "    jordangarrett@ucsb.edu\n",
    "    ====================================================================\n",
    "    \"\"\"\n",
    "\n",
    "    data_dir =  os.path.join(os.getcwd(),'Yelp_Data\\\\')\n",
    "\n",
    "    #check to see if any cities in the list have previously been searched\n",
    "    if os.path.exists(data_dir+'cities_list.txt'):\n",
    "        \n",
    "        try:\n",
    "            prev_cities = pickle.load(open(data_dir+'cities_list.txt','rb'))\n",
    "        except EOFError:\n",
    "            prev_cities = []\n",
    "            \n",
    "        city_names = [city for city in city_names if city not in prev_cities]\n",
    "\n",
    "\n",
    "        if city_names:\n",
    "            print(f'Searching Cities: {city_names}')\n",
    "        else:\n",
    "            print('All cities have already been searched')\n",
    "            return\n",
    "\n",
    "\n",
    "    yelp_api = YelpAPI(api_key)\n",
    "\n",
    "    # add in pauses to prevent stop errors from too much scraping\n",
    "    time.sleep(3)\n",
    "\n",
    "    all_business_df = pd.DataFrame()\n",
    "    all_reviews_df = pd.DataFrame()\n",
    "\n",
    "    for iCity in city_names:\n",
    "\n",
    "        # we can play around with the limit and offset parameters \n",
    "        # to control the number of results and what item to start the pull on\n",
    "        search_results = yelp_api.search_query(term = 'Internet Service Providers',\n",
    "                                               location = iCity, limit = 50)\n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "        business_df = pd.DataFrame.from_dict(search_results['businesses'])\n",
    "\n",
    "\n",
    "        # drop the phone, display_phone, transactions, is_closed, and image_url columns\n",
    "        # we shouldn't need them\n",
    "        unecessary_cols = ['phone', 'display_phone', 'transactions', 'is_closed','image_url']\n",
    "\n",
    "\n",
    "        print(iCity)\n",
    "        business_df2 = business_df.drop(unecessary_cols,1)\n",
    "\n",
    "        #loop through businesses\n",
    "        reviews = dict()\n",
    "\n",
    "        reviews_df = pd.DataFrame()\n",
    "        for iBiz, biz_id in enumerate(business_df2.loc[:,'id']):\n",
    "            business_name = business_df2['name'][iBiz]\n",
    "\n",
    "            #can only get 3 reviews through yelp api\n",
    "            #BUT...we have the url...which means it should be easy to \"not legally\" scrape\n",
    "            reviews[business_name] = yelp_api.reviews_query(biz_id)\n",
    "\n",
    "            # temporary data frame we can use that will be appended to a master one later\n",
    "            temp_df = pd.DataFrame.from_dict(reviews[business_name]['reviews'])\n",
    "\n",
    "            temp_df = temp_df.drop('user',1)\n",
    "\n",
    "            # add column for ISP provider\n",
    "            temp_df.insert(0,'ISP_name',business_name)\n",
    "\n",
    "            # add column for business id\n",
    "            temp_df.insert(1,'business_id',biz_id)\n",
    "\n",
    "            # add column for business location\n",
    "            temp_df.insert(6,'location',\n",
    "                str(business_df2[business_df2['id'] == biz_id]['location'].item()['display_address']))\n",
    "\n",
    "\n",
    "            temp_df = temp_df.rename(columns = {\"id\":\"rev_id\"})\n",
    "\n",
    "            reviews_df = reviews_df.append(temp_df,ignore_index = True)\n",
    "\n",
    "\n",
    "            all_business_df = all_business_df.append(business_df2, ignore_index=True)\n",
    "            all_reviews_df = all_reviews_df.append(reviews_df,ignore_index=True)\n",
    "\n",
    "    # Save data\n",
    "    # if no previous files, just save the data. if previous files, append\n",
    "    if business_data == None and business_reviews == None:\n",
    "        all_business_df.to_csv(data_dir+'businesses.csv', index=False)\n",
    "        all_reviews_df.to_csv(data_dir+'businesses_reviews.csv', index = False)\n",
    "\n",
    "    else: #append data to previous loaded files\n",
    "\n",
    "        prev_business_df = pd.read_csv(data_dir+business_data)\n",
    "\n",
    "        prev_reviews_df = pd.read_csv(data_dir+business_reviews)\n",
    "\n",
    "        new_business_df = prev_business_df.append(all_business_df, ignore_index = True)\n",
    "        new_reviews_df = prev_reviews_df.append(all_reviews_df, ignore_index = True)\n",
    "\n",
    "        new_business_df.to_csv(data_dir+'businesses.csv', index=False)\n",
    "        new_reviews_df.to_csv(data_dir+'businesses_reviews.csv', index = False)\n",
    "\n",
    "\n",
    "    # Save previous cities to ensure that we aren't looking at cities previously searched\n",
    "    pickle.dump(city_names,open(data_dir+\"cities_list.txt\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching Cities: ['Adelanto, CA', 'Agoura Hills, CA', 'Alameda, CA', 'Albany, CA', 'Alhambra, CA', 'Aliso Viejo, CA', 'Alturas, CA', 'Amador City, CA', 'American Canyon, CA', 'Anaheim, CA']\n",
      "Adelanto, CA\n",
      "Agoura Hills, CA\n",
      "Alameda, CA\n",
      "Albany, CA\n",
      "Alhambra, CA\n",
      "Aliso Viejo, CA\n",
      "Alturas, CA\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['phone' 'display_phone' 'transactions' 'is_closed' 'image_url'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-8c0053f69d02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcity_chunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfirstHalf_chunks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mYelp_ScrapeISP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcity_chunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-9d16d48d69c6>\u001b[0m in \u001b[0;36mYelp_ScrapeISP\u001b[1;34m(api_key, city_names, business_data, business_reviews)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miCity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mbusiness_df2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbusiness_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munecessary_cols\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;31m#loop through businesses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jordan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4160\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[1;36m1.0\u001b[0m     \u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4161\u001b[0m         \"\"\"\n\u001b[1;32m-> 4162\u001b[1;33m         return super().drop(\n\u001b[0m\u001b[0;32m   4163\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4164\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jordan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3882\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3883\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3884\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3885\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3886\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jordan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3916\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3917\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3918\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3919\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3920\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jordan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   5276\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5277\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5278\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5279\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5280\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['phone' 'display_phone' 'transactions' 'is_closed' 'image_url'] not found in axis\""
     ]
    }
   ],
   "source": [
    "#get the api key by creating an account on yelp and then clicking on Create App. Fill out\n",
    "#form and it will generate a key for you.\n",
    "api_key = 'Gxa0LqhgTU-G3sf9RuA_kt5dHTxgH-m5BNMdM-0TpN56PYRFdEnoj811SGiz9O2-a5TazMI5VpOzzBH91ZMX9p4PJ1K-ALQ0VSnuuL3t4Yt97lrV-3dBdNikmVC3X3Yx'\n",
    "ca_cities_df = pd.read_csv(os.path.join(os.getcwd(),'Yelp_Data\\\\cal_cities_lat_long.csv'))\n",
    "ca_cities = ca_cities_df['Name'] + ', CA'\n",
    "\n",
    "first_half_cities = ca_cities[0:round(len(ca_cities)/2)]\n",
    "second_half_cities = ca_cities[round(len(ca_cities)/2):len(ca_cities)]\n",
    "\n",
    "first_half_cities = first_half_cities.to_list()\n",
    "\n",
    "\n",
    "# first half of cities has 230 cities.\n",
    "# chunk them when passing through function to avoid scraping errors\n",
    "# using 23 chunks of 10 cities\n",
    "n = 10 # number of cities in each chunk\n",
    "\n",
    "firstHalf_chunks = [first_half_cities[i * n:(i + 1) * n] for i in range((len(first_half_cities) + n - 1) // n)]\n",
    "\n",
    "for city_chunk in firstHalf_chunks:\n",
    "\n",
    "    Yelp_ScrapeISP(api_key,city_chunk)\n",
    "\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching Cities: ['Santa Barbara, CA', 'Walnut, CA', 'Santa Clarita, CA']\n",
      "Santa Barbara, CA\n",
      "Walnut, CA\n",
      "Santa Clarita, CA\n"
     ]
    }
   ],
   "source": [
    "Yelp_ScrapeISP(api_key,['Santa Barbara, CA','Walnut, CA','Santa Clarita, CA'], None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
