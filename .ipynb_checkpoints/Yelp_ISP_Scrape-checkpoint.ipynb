{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yelpapi import YelpAPI\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir =  os.path.join(os.getcwd(),'Yelp_Data\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'Gxa0LqhgTU-G3sf9RuA_kt5dHTxgH-m5BNMdM-0TpN56PYRFdEnoj811SGiz9O2-a5TazMI5VpOzzBH91ZMX9p4PJ1K-ALQ0VSnuuL3t4Yt97lrV-3dBdNikmVC3X3Yx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_api = YelpAPI(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part is going to be a pain. We can loop through this and get \n",
    "# results for each area, but there is a limit on the api request per day \n",
    "# per 30 days. Maybe just focus on major cities?\n",
    "\n",
    "businesses_location = 'Santa Barbara, CA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can play around with the limit and offset parameters \n",
    "# to control the number of results and what item to start the pull on\n",
    "search_results = yelp_api.search_query(term='Internet Service Providers',\n",
    "                                       location=businesses_location,\n",
    "                                      limit = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df = pd.DataFrame.from_dict(search_results['businesses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the phone, display_phone, transactions, is_closed, and image_url columns\n",
    "# we shouldn't need them\n",
    "unecessary_cols = ['phone', 'display_phone', 'transactions', 'is_closed', \n",
    "                   'image_url']\n",
    "\n",
    "\n",
    "business_df2 = business_df.drop(unecessary_cols,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through businesses\n",
    "business_reviews = dict()\n",
    "for iBiz, biz_id in enumerate(business_df2.loc[:,'id']):\n",
    "    business_name = business_df2['name'][iBiz]\n",
    "    \n",
    "    #can only get 3 reviews through yelp api\n",
    "    #BUT...we have the url...which means it should be easy to \"not legally\" scrape\n",
    "    business_reviews[business_name] = yelp_api.reviews_query(biz_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "businessRev_df = pd.DataFrame()\n",
    "\n",
    "for biz in business_reviews.keys():\n",
    "    \n",
    "    # temporary data frame we can use that will be appended to a master one later\n",
    "    temp_df = pd.DataFrame.from_dict(business_reviews[biz]['reviews'])\n",
    "    \n",
    "    temp_df = temp_df.drop('user',1)\n",
    "    \n",
    "    # add column for ISP provider\n",
    "    temp_df.insert(0,'ISP_name',biz)\n",
    "    \n",
    "    # add column for business id\n",
    "    temp_df.insert(1,'business_id',\n",
    "                   business_df[business_df['name'] == biz]['id'].item())\n",
    "    \n",
    "    temp_df.insert(6,'location',\n",
    "                       str(business_df2[business_df2['name'] == biz]['location'].item()['display_address']))\n",
    "    \n",
    "    temp_df = temp_df.rename(columns = {\"id\":\"rev_id\"})\n",
    "    \n",
    "    businessRev_df = businessRev_df.append(temp_df,ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "business_df2.to_csv(data_dir+'businesses.csv', index=False)\n",
    "businessRev_df.to_csv(data_dir+'businesses_reviews.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\PSTAT_235\\\\cal_cities_lat_long.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d1e184564643>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;31m#form and it will generate a key for you.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[0mapi_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'slks'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m \u001b[0mca_cities_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'cal_cities_lat_long.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m \u001b[0mca_cities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mca_cities_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Name'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', CA'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jordan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jordan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jordan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 936\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jordan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1166\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1167\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1168\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1169\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1170\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jordan\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1996\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1998\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1999\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\PSTAT_235\\\\cal_cities_lat_long.csv'"
     ]
    }
   ],
   "source": [
    "from yelpapi import YelpAPI\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def Yelp_ScrapeISP (api_key,city_names,business_data = 'businesses.csv',\n",
    "                    business_reviews = 'businesses_reviews.csv'):\n",
    "    \n",
    "    \"\"\"\n",
    "    ====================================================================\n",
    "    Version: 1.0.0\n",
    "    Date: Tue 24 Nov 2020\n",
    "\n",
    "    Purposes: Search and save yelp data about internet service providers\n",
    "    within a region. \n",
    "\n",
    "    Input:\n",
    "        Required:\n",
    "            api_key = Api key assigned by yelp fusion\n",
    "            city_names = List of locations for internet service providers\n",
    "        Opitional:\n",
    "            business_data = .csv file containing data from previous searches\n",
    "            business_reviews = .csv file containing reviews of businesses from\n",
    "                                previous searches\n",
    "\n",
    "    Output:\n",
    "        'businesses.csv' containing information about internet \n",
    "            service providers\n",
    "\n",
    "        'businesses_reviews.csv' containing reviews of internet service\n",
    "            providers in the businesses.csv file\n",
    "\n",
    "        'cities_list.txt' list of previous cities that have been searched.\n",
    "            Data will only be extracted for cities that have not been\n",
    "            previously searched.\n",
    "\n",
    "\n",
    "    Example: \n",
    "    ca_cities_df = pd.read_csv('cal_cities_lat_long.csv')\n",
    "    ca_cities = ca_cities_df['Name'] + ', CA'\n",
    "\n",
    "    api_key = XIXIXIXLXJO\n",
    "\n",
    "    Yelp_ScrapeISP(api_key,ca_cities)\n",
    "\n",
    "\n",
    "    Author: Jordan Garrett\n",
    "    jordangarrett@ucsb.edu\n",
    "    ====================================================================\n",
    "    \"\"\"\n",
    "\n",
    "    data_dir =  os.path.join(os.getcwd(),'Yelp_Data\\\\')\n",
    "    \n",
    "    #check to see if any cities in the list have previously been searched\n",
    "    if os.path.exists(data_dir+'cities_list.txt'):\n",
    "    \tprev_cities = pickle.load(open(data_dir+'cities_list.txt','rb'))\n",
    "    \tcity_names = [city for city in prev_cities if city not in city_names]\n",
    "\n",
    "\n",
    "    if city_names:\n",
    "    \tprint(f'Searching Cities: {cities}')\n",
    "    else:\n",
    "    \tprint('All cities have already been searched')\n",
    "    \treturn\n",
    "\n",
    "\n",
    "    yelp_api = YelpAPI(api_key)\n",
    "\n",
    "    # add in pauses to prevent stop errors from too much scraping\n",
    "    time.sleep(3)\n",
    "    \n",
    "    all_business_df = pd.DataFrame()\n",
    "    all_reviews_df = pd.DataFrame()\n",
    "\n",
    "    for iCity in cities:\n",
    "        \n",
    "        # we can play around with the limit and offset parameters \n",
    "        # to control the number of results and what item to start the pull on\n",
    "        search_results = yelp_api.search_query(term = 'Internet Service Providers',\n",
    "                                               location = iCity, limit = 50)\n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "        business_df = pd.DataFrame.from_dict(search_results['businesses'])\n",
    "\n",
    "\n",
    "        # drop the phone, display_phone, transactions, is_closed, and image_url columns\n",
    "        # we shouldn't need them\n",
    "        unecessary_cols = ['phone', 'display_phone', 'transactions', 'is_closed','image_url']\n",
    "\n",
    "\n",
    "        business_df2 = business_df.drop(unecessary_cols,1)\n",
    "\n",
    "        #loop through businesses\n",
    "        reviews = dict()\n",
    "\n",
    "        reviews_df = pd.DataFrame()\n",
    "        for iBiz, biz_id in enumerate(business_df2.loc[:,'id']):\n",
    "            business_name = business_df2['name'][iBiz]\n",
    "\n",
    "            #can only get 3 reviews through yelp api\n",
    "            #BUT...we have the url...which means it should be easy to \"not legally\" scrape\n",
    "            reviews[business_name] = yelp_api.reviews_query(biz_id)\n",
    "\n",
    "            # temporary data frame we can use that will be appended to a master one later\n",
    "            temp_df = pd.DataFrame.from_dict(reviews[business_name]['reviews'])\n",
    "\n",
    "            temp_df = temp_df.drop('user',1)\n",
    "\n",
    "            # add column for ISP provider\n",
    "            temp_df.insert(0,'ISP_name',business_name)\n",
    "\n",
    "            # add column for business id\n",
    "            temp_df.insert(1,'business_id',biz_id)\n",
    "\n",
    "            # add column for business location\n",
    "            temp_df.insert(6,'location',\n",
    "                           str(business_df2[business_df2['id'] == biz_id]['location'].item()['display_address']))\n",
    "\n",
    "\n",
    "            temp_df = temp_df.rename(columns = {\"id\":\"rev_id\"})\n",
    "\n",
    "            reviews_df = reviews_df.append(temp_df,ignore_index = True)\n",
    "\n",
    "\n",
    "        all_business_df = all_business_df.append(business_df2, ignore_index=True)\n",
    "        all_reviews_df = all_reviews_df.append(reviews_df,ignore_index=True)\n",
    "\n",
    "    # Save data\n",
    "    # if no previous files, just save the data. if previous files, append\n",
    "    if business_data == None and business_reviews == None:\n",
    "        all_business_df.to_csv(data_dir+'businesses.csv', index=False)\n",
    "        all_reviews_df.to_csv(data_dir+'businesses_reviews.csv', index = False)\n",
    "\n",
    "    else: #append data to previous loaded files\n",
    "\n",
    "        prev_business_df = pd.read_csv(data_dir+business_data)\n",
    "\n",
    "        prev_reviews_df = pd.read_csv(data_dir+business_reviews)\n",
    "\n",
    "        new_business_df = prev_business_df.append(all_business_df, ignore_index = True)\n",
    "        new_reviews_df = prev_reviews_df.append(all_reviews_df, ignore_index = True)\n",
    "\n",
    "        new_business_df.to_csv(data_dir+'businesses.csv', index=False)\n",
    "        new_reviews_df.to_csv(data_dir+'businesses_reviews.csv', index = False)\n",
    "\n",
    "\n",
    "    # Save previous cities to ensure that we aren't looking at cities previously searched\n",
    "    pickle.dump(city_names,open(data_dir+\"cities_list.txt\",\"wb\"))\n",
    "\n",
    "\n",
    "#get the api key by creating an account on yelp and then clicking on Create App. Fill out\n",
    "#form and it will generate a key for you.\n",
    "api_key = 'slks'\n",
    "ca_cities_df = pd.read_csv(os.path.join(os.getcwd(),'Yelp_Data\\\\cal_cities_lat_long.csv'))\n",
    "ca_cities = ca_cities_df['Name'] + ', CA'\n",
    "\n",
    "first_half_cities = ca_cities[0:round(len(ca_cities)/2)]\n",
    "second_half_cities = ca_cities[round(len(ca_cities)/2):len(ca_cities)]\n",
    "\n",
    "business_dataFile = 'businesses.csv'\n",
    "business_reviewsFile = 'businesses_reviews.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yelp_ScrapeISP(api_key,cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
