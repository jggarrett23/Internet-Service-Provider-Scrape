{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> PSTAT 235 Group Project: Internet Service Providers' Review Ratings Predictions </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Steps\n",
    "* Drop unnecessary columns\n",
    "* Convert ratings into integers\n",
    "* <span style = \"color:red\"> **(TO DO)** Remove duplicate reviews (use rev_ids)</span>\n",
    "* Natural Language Processing (NLP)\n",
    "    * Remove Punctuations\n",
    "    * Tokenize words\n",
    "    * Remove stop words\n",
    "    * Word2Vec\n",
    "    * <span style = \"color:red\"> **(TO DO)** Scale Features with StandardScaler</span>\n",
    "* Sentiment Extraction (use package vaderSentiment on full reviews)\n",
    "\n",
    "#### Considerations\n",
    "* Optimize pipeline using RDDs. Currently only using pyspark dataframes because they are easier to use for structured data, but when applying transformations such as Word2Vec it takes super long. And this is only with half the data. MLib has Word2Vec for RDDs, but the results are not per row like with the ml version (which requires a dataframe). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vaderSentiment in /opt/conda/lib/python3.7/site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from vaderSentiment) (2.22.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->vaderSentiment) (1.25.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->vaderSentiment) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->vaderSentiment) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->vaderSentiment) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install vaderSentiment\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, Word2Vec\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import IntegerType\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"isp_analysis\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#businesses_dataFile = 'businesses.csv'\n",
    "reviews_dataFile = 'Yelp_Data/businesses_reviews.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First read in as dataframe to drop unnecessary columns\n",
    "\n",
    "#businessesRDD = sc.textFile(businesses_dataFile)\n",
    "reviews_df = spark.read.csv(reviews_dataFile, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['time_create','url','rev_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns to reduce overhead later on. \n",
    "# Maybe drop location from business reviews and put lat/long coordinates from businesses_df\n",
    "reviews_df2 = reviews_df.drop('time_created','url','rev_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the ratings column to an integer\n",
    "reviews_df2 = reviews_df2.withColumn('rating', reviews_df2['rating'].cast(IntegerType()))\n",
    "\n",
    "# Rename text column to reviews\n",
    "reviews_df2 = reviews_df2.withColumnRenamed('text','reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove puncutation. Have to do this before tokenization because of noninterpretable errors\n",
    "removePunctuation_udf = udf(lambda x: re.sub('[^\\w\\s]','',str(x)))\n",
    "\n",
    "reviews_noPunc = reviews_df2.withColumn('non_punc', removePunctuation_udf('reviews'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the reviews column\n",
    "tokenizer = Tokenizer(inputCol = 'non_punc', outputCol = 'words')\n",
    "reviews_tokenized = tokenizer.transform(reviews_noPunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol = 'words', outputCol = 'filtered')\n",
    "reviews_stopsRemoved = remover.transform(reviews_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+--------------------+\n",
      "|rating|            ISP_name|            filtered|             reviews|\n",
      "+------+--------------------+--------------------+--------------------+\n",
      "|     5|Comcast Service C...|[review, service,...|This review is fo...|\n",
      "|     1|Comcast Service C...|[goes, xfinity, c...|This goes for all...|\n",
      "|     1|Comcast Service C...|[im, giving, one,...|I'm only giving o...|\n",
      "|     1|ClearView Communi...|[clearview, disho...|ClearView is a di...|\n",
      "|     1|             CONX TV|[dish, tv, , disc...|Dish tv  discount...|\n",
      "+------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select necessary columns and convert to RDD.\n",
    "# saving raw reviews for sentiment conversion\n",
    "reviews_df3 = reviews_stopsRemoved.select('rating','ISP_name','filtered','reviews')\n",
    "\n",
    "reviews_df3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion of Reviews Took: 13.5 Seconds\n"
     ]
    }
   ],
   "source": [
    "# Convert reviews to numerical features using Word2Vec\n",
    "# This step will take a while\n",
    "#start = time.time()\n",
    "# set vector size arbitrarily. we dont want to make the model too large \n",
    "#word2vec = Word2Vec(vectorSize = 5, inputCol = 'filtered', outputCol = 'word_vecs')\n",
    "#model = word2vec.fit(reviews_df3)\n",
    "\n",
    "#reviews_wordVec = model.transform(reviews_df3)\n",
    "\n",
    "#print(f'Conversion of Reviews Took: {round(time.time()-start,2)} Seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "polarity_udf = udf(lambda x: analyzer.polarity_scores(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviews_final = reviews_wordVec.withColumn('valence', polarity_udf('reviews'))\n",
    "reviews_final = reviews_df3.withColumn('valence', polarity_udf(reviews_df3['filtered']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to RDD for faster modeling?\n",
    "#processed_reviewsRDD = reviews_final.rdd.map(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+\n",
      "|valence                                  |\n",
      "+-----------------------------------------+\n",
      "|{neg=0.0, pos=0.0, compound=0.0, neu=1.0}|\n",
      "|{neg=0.0, pos=0.0, compound=0.0, neu=1.0}|\n",
      "|{neg=0.0, pos=0.0, compound=0.0, neu=1.0}|\n",
      "|{neg=0.0, pos=0.0, compound=0.0, neu=1.0}|\n",
      "|{neg=0.0, pos=0.0, compound=0.0, neu=1.0}|\n",
      "|{neg=0.0, pos=0.0, compound=0.0, neu=1.0}|\n",
      "|{neg=0.0, pos=0.0, compound=0.0, neu=1.0}|\n",
      "|{neg=0.0, pos=0.0, compound=0.0, neu=1.0}|\n",
      "|{neg=0.0, pos=0.0, compound=0.0, neu=1.0}|\n",
      "|{neg=0.0, pos=0.0, compound=0.0, neu=1.0}|\n",
      "+-----------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "reviews_final.select('valence').show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion of Reviews Took: 17.61 Seconds\n"
     ]
    }
   ],
   "source": [
    "# Convert reviews to numerical features using Word2Vec\n",
    "# This step will take a while\n",
    "start = time.time()\n",
    "# set vector size arbitrarily. we dont want to make the model too large \n",
    "word2vec = Word2Vec(vectorSize = 100, inputCol = 'filtered', outputCol = 'word_vecs')\n",
    "model = word2vec.fit(reviews_df3)\n",
    "\n",
    "reviews_wordVec = model.transform(reviews_df3)\n",
    "\n",
    "print(f'Conversion of Reviews Took: {round(time.time()-start,2)} Seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+--------------------+--------------------+\n",
      "|rating|            ISP_name|            filtered|             reviews|           word_vecs|\n",
      "+------+--------------------+--------------------+--------------------+--------------------+\n",
      "|     5|Comcast Service C...|[review, service,...|This review is fo...|[-0.0855886761564...|\n",
      "|     1|Comcast Service C...|[goes, xfinity, c...|This goes for all...|[-0.0182209065055...|\n",
      "|     1|Comcast Service C...|[im, giving, one,...|I'm only giving o...|[-0.0729191299962...|\n",
      "|     1|ClearView Communi...|[clearview, disho...|ClearView is a di...|[-0.0362489023669...|\n",
      "|     1|             CONX TV|[dish, tv, , disc...|Dish tv  discount...|[0.02664551820213...|\n",
      "|     3|            Spectrum|[hallelujah, open...|***HALLELUJAH THE...|[-0.1190661154105...|\n",
      "|     1|            Spectrum|[trust, door, doo...|DO NOT TRUST the ...|[-0.0548134035430...|\n",
      "|     4|            Spectrum|[looking, charter...|If you are lookin...|[-0.2015117227386...|\n",
      "|     2|            Spectrum|[fastest, interne...|The fastest inter...|[-0.0065703579457...|\n",
      "|     1|            Spectrum|[racket, , compan...|What a racket . T...|[-0.1128886457532...|\n",
      "+------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews_wordVec.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "from pyspark.sql.functions import length\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.ml.regression import LinearRegression # note this is from the ML package\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline  \n",
    "\n",
    "## PREPROCESSNG\n",
    "# Remove ratings == null\n",
    "reviews_df4 = reviews_df3.filter(reviews_df3['rating'].isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+--------------------+------------+-------+\n",
      "|rating|            ISP_name|            filtered|             reviews|reviewLength|exCount|\n",
      "+------+--------------------+--------------------+--------------------+------------+-------+\n",
      "|     5|Comcast Service C...|[review, service,...|This review is fo...|         152|      0|\n",
      "|     1|Comcast Service C...|[goes, xfinity, c...|This goes for all...|         159|      0|\n",
      "|     1|Comcast Service C...|[im, giving, one,...|I'm only giving o...|         154|      2|\n",
      "|     1|ClearView Communi...|[clearview, disho...|ClearView is a di...|         154|      0|\n",
      "|     1|             CONX TV|[dish, tv, , disc...|Dish tv  discount...|         159|      0|\n",
      "|     3|            Spectrum|[hallelujah, open...|***HALLELUJAH THE...|         157|      0|\n",
      "|     1|            Spectrum|[trust, door, doo...|DO NOT TRUST the ...|         159|      5|\n",
      "|     4|            Spectrum|[looking, charter...|If you are lookin...|         145|      0|\n",
      "|     2|            Spectrum|[fastest, interne...|The fastest inter...|         156|      0|\n",
      "|     1|            Spectrum|[racket, , compan...|What a racket . T...|         159|      0|\n",
      "|     1|            Spectrum|[spectrum, added,...|Spectrum added Sp...|         154|      0|\n",
      "|     4|PLUMAS-SIERRA Tel...|[business, owner,...|As a business own...|         155|      0|\n",
      "|     5|PLUMAS-SIERRA Tel...|[get, world, clas...|I get world class...|         158|      0|\n",
      "|     5|              Net NV|[net, nv, service...|I have had Net NV...|         159|      0|\n",
      "|     5|              Net NV|[one, best, inter...|This is one of th...|         157|      0|\n",
      "+------+--------------------+--------------------+--------------------+------------+-------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## FEATURE ENGINEERING\n",
    "# Create column for length of review\n",
    "reviews_df4 = reviews_df4.withColumn('reviewLength', length(reviews_df4.reviews))\n",
    "\n",
    "# Create a column for the count of a specific word, here an \"!\"\n",
    "word = '!'\n",
    "wordCount_udf = udf(lambda col: len([x for x in col if x == word]), IntegerType())\n",
    "reviews_df4 = reviews_df4.withColumn('exCount', wordCount_udf(reviews_df4.reviews))\n",
    "\n",
    "reviews_df4.show(15)\n",
    "\n",
    "# One hot encoding for ISP_name\n",
    "SI = StringIndexer(inputCol=\"ISP_name\", outputCol=\"nameIndex\")\n",
    "OHE = OneHotEncoder(inputCol=\"nameIndex\", outputCol=\"nameDummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure pipeline stages \n",
    "# I only separated them to show features we created versus those we derive with functions\n",
    "VA = VectorAssembler(inputCols=['reviewLength', 'exCount']+['nameDummy'], outputCol=\"features\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODEL BUILDING\n",
    "# Create parameters of model\n",
    "seed = 314\n",
    "train_test = [0.6, 0.4]\n",
    "\n",
    "train_data1, test_data1 = reviews_df4.randomSplit(train_test, seed)\n",
    "\n",
    "maxIter=10\n",
    "regParam=0.3\n",
    "elasticNetParam=0.8\n",
    "\n",
    "LR1 = LinearRegression(featuresCol='features', labelCol='rating',\n",
    "                       maxIter=maxIter, regParam=regParam, elasticNetParam=elasticNetParam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+--------------------+------------+-------+---------+-----------------+--------------------+------------------+\n",
      "|rating|            ISP_name|            filtered|             reviews|reviewLength|exCount|nameIndex|        nameDummy|            features|        prediction|\n",
      "+------+--------------------+--------------------+--------------------+------------+-------+---------+-----------------+--------------------+------------------+\n",
      "|     1|1 On 1 Communicat...|[terrible, custom...|Terrible customer...|         156|      0|    444.0|(817,[444],[1.0])|(819,[0,446],[156...|3.6500602312182635|\n",
      "|     1|995 Web Internet ...|[couldnt, provide...|Couldn't provide ...|         157|      0|    686.0|(817,[686],[1.0])|(819,[0,688],[157...|3.6500602312182635|\n",
      "|     1|         ABS-CBNnow!|[im, sure, got, n...|I'm not sure how ...|         159|      0|    543.0|(817,[543],[1.0])|(819,[0,545],[159...|3.6500602312182635|\n",
      "|     1|         ABS-CBNnow!|[im, sure, got, n...|I'm not sure how ...|         159|      0|    543.0|(817,[543],[1.0])|(819,[0,545],[159...|3.6500602312182635|\n",
      "|     1|   ACA Group Service|[ppl, r, scam, ar...|These ppl R scam ...|         150|      1|    164.0|(817,[164],[1.0])|(819,[0,1,166],[1...|3.6500602312182635|\n",
      "+------+--------------------+--------------------+--------------------+------------+-------+---------+-----------------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build pipeline\n",
    "pipeline = Pipeline(stages=[SI, OHE, VA, LR1])\n",
    "\n",
    "# Create model\n",
    "model1 = pipeline.fit(train_data1)\n",
    "\n",
    "# Make predicions\n",
    "preds1 = model1.transform(test_data1)\n",
    "\n",
    "preds1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
